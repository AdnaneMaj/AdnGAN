{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from math import log2\n",
    "#tensorboard\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "#tqdm\n",
    "from tqdm import tqdm\n",
    "\n",
    "from AdnGAN import Generator_pro, Discriminator_pro,tools,get_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Equalized learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w_f=w_i\\sqrt{\\frac{2}{k*k**c}}$$\n",
    "\n",
    "* k : Kernel size\n",
    "* c : in channels\n",
    "* w : weigh\n",
    "\n",
    "**Weight Initialization:** Initializing weights from a normal distribution means that the initial weights have varying magnitudes. In convolutional neural networks (CNNs), each weight corresponds to a feature detector, and these detectors might have very different magnitudes initially.\n",
    "\n",
    "**Consequence of Weight Initialization:** During the forward pass through a layer, the input is convolved with these weights. If the weights have varying magnitudes, the output of the convolution will also have varying magnitudes. This can lead to instability in training because the network might respond more strongly to some input features than to others, simply due to the magnitude of the weights.\n",
    "\n",
    "**Normalization:** By normalizing the weights, you ensure that each weight vector has a consistent magnitude. This makes the network more robust and less sensitive to the scale of the input features.\n",
    "\n",
    "**Scaling:** However, when you normalize the weights, you might inadvertently reduce the overall magnitude of the signal passing through the layer. To counteract this, you scale the input by a factor (self.scale in your code). This scaling factor ensures that the signal's magnitude remains approximately constant despite the weight normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#equalized leaning rate for a conv2d\n",
    "class WSConv2d(nn.Module):\n",
    "    def __init__(self,in_channels,out_channels,kernel_size=3,stride=1,padding=1,gain=2):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(in_channels,out_channels,kernel_size,stride,padding)\n",
    "        self.scale = (gain / (in_channels * (kernel_size ** 2))) ** 0.5\n",
    "        self.bias = self.conv.bias #We are copying the bias of the current conv layer, we don't want the bias to be scaled, only the weights\n",
    "        self.conv.bias = None #we remove the bias (i don't fucking understand why hhhhh)\n",
    "\n",
    "        #initlaise conv layer\n",
    "        nn.init.normal_(self.conv.weight) #initalise from normal disterbution\n",
    "        nn.init.zeros_(self.bias) #initialise with zeros\n",
    "\n",
    "    def forward(self,x):\n",
    "        return self.conv(x*self.scale)+self.bias.view(1,self.bias.shape[0],1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pixel normalsiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pixel norm class (instead of batch norm)\n",
    "class PixelNorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        #epsilon : 1e-8\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x/torch.sqrt(torch.mean(x**2,dim=1,keepdim=True)+1e-8) #dim=1 : The mean accros the channels since dim=0 correspend to the batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConvBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    This block will be used in the G and D\n",
    "    We will use the conv2D using the equalized learning rate (initialisation)\n",
    "    \"\"\"\n",
    "    def __init__(self,in_channels,out_channels,use_pixelnorm=True):\n",
    "        super().__init__()\n",
    "        self.conv1 = WSConv2d(in_channels,out_channels)\n",
    "        self.conv2 = WSConv2d(out_channels,out_channels)\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "        self.pn = PixelNorm()\n",
    "        self.use_pn = use_pixelnorm\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.leaky(self.conv1(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        x = self.leaky(self.conv2(x))\n",
    "        x = self.pn(x) if self.use_pn else x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self,z_dim,in_channels,img_channels=3):\n",
    "        super().__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            PixelNorm(),\n",
    "            nn.ConvTranspose2d(z_dim,in_channels,kernel_size=4,stride=1,padding=0), #It take 1*1 -> 4*4\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels,in_channels,kernel_size=3,stride=1,padding=1),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            PixelNorm(),\n",
    "        )\n",
    "\n",
    "        self.initial_rgb = WSConv2d(in_channels,img_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.prog_blocks,self.rgb_layers = nn.ModuleList([]),nn.ModuleList([self.initial_rgb]) #progressive blocs and rgb_layers\n",
    "\n",
    "        for i in range(len(factors)-1):\n",
    "            #factors[i] => factors[i+1]\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i+1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c,conv_out_c))\n",
    "            self.rgb_layers.append(WSConv2d(conv_out_c,img_channels,kernel_size=1,stride=1,padding=0))\n",
    "\n",
    "    def fade_in(self,alpha,upscaled,generated):\n",
    "        return torch.tanh(alpha*generated+(1-alpha)*upscaled) #[-1,1]\n",
    "\n",
    "    def forward(self,x,alpha,steps): # step=0(4*4) steps=1 (8*8) ...\n",
    "        out = self.initial(x) #4*4\n",
    "        if steps == 0 :\n",
    "            return self.initial_rgb(out)\n",
    "        \n",
    "        for step in range(steps):\n",
    "            upscaled = F.interpolate(out,scale_factor=2,mode=\"nearest\") #upscale\n",
    "            out = self.prog_blocks[step](upscaled) #run throught the prog block\n",
    "\n",
    "        final_upscaled = self.rgb_layers[steps-1](upscaled)\n",
    "        final_out = self.rgb_layers[steps](out)\n",
    "        return self.fade_in(alpha,final_upscaled,final_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self,in_channels,img_channels=3):\n",
    "        super().__init__()\n",
    "        self.prog_blocks,self.rgb_layers = nn.ModuleList([]),nn.ModuleList([])\n",
    "        self.leaky = nn.LeakyReLU(0.2)\n",
    "\n",
    "        for i in range(len(factors)-1,0,-1):\n",
    "            conv_in_c = int(in_channels*factors[i])\n",
    "            conv_out_c = int(in_channels*factors[i-1])\n",
    "            self.prog_blocks.append(ConvBlock(conv_in_c,conv_out_c,use_pixelnorm=False))\n",
    "            self.rgb_layers.append(WSConv2d(img_channels,conv_in_c,kernel_size=1,stride=1,padding=0))\n",
    "\n",
    "        #This for 4*4 img resolution\n",
    "        self.initial_rgb = WSConv2d(img_channels,in_channels,kernel_size=1,stride=1,padding=0)\n",
    "        self.rgb_layers.append(self.initial_rgb)\n",
    "        self.avg_pool = nn.AvgPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        #block for 4*4 resolution\n",
    "        self.final_block = nn.Sequential(\n",
    "            WSConv2d(in_channels+1,in_channels,kernel_size=3,stride=1,padding=1), #513*4*4 to 512*4*4 (last block)\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels,in_channels,kernel_size=4,stride=1,padding=0),\n",
    "            nn.LeakyReLU(0.2),\n",
    "            WSConv2d(in_channels,1,kernel_size=1,stride=1,padding=0) #In the paper they did a linear layer (he said that it's the same thing)\n",
    "        )\n",
    "\n",
    "\n",
    "    def fade_in(self,alpha,downscaled,out):\n",
    "        return alpha*out+(1-alpha)*downscaled\n",
    "\n",
    "    def minibatch_std(self,x):\n",
    "        batch_statistics = torch.std(x,dim=0).mean().repeat(x.shape[0],1,x.shape[2],x.shape[3]) #The std of every example of x N*C*H*W ==> N\n",
    "        return torch.cat([x,batch_statistics],dim=1)\n",
    "\n",
    "    def forward(self,x,alpha,steps): # steps = 0 (4*4), steps=1 (8*8) , etc6\n",
    "        cur_step = len(self.prog_blocks)-steps\n",
    "        out = self.leaky(self.rgb_layers[cur_step](x))\n",
    "\n",
    "        if steps == 0:\n",
    "            out = self.minibatch_std(out)\n",
    "            return self.final_block(out).view(out.shape[0],-1)\n",
    "        \n",
    "        downscaled = self.leaky(self.rgb_layers[cur_step+1](self.avg_pool(x)))\n",
    "        out = self.avg_pool(self.prog_blocks[cur_step](out))\n",
    "        out = self.fade_in(alpha,downscaled,out)\n",
    "\n",
    "        for step in range(cur_step+1,len(self.prog_blocks)):\n",
    "            out = self.prog_blocks[step](out)\n",
    "            out = self.avg_pool(out)\n",
    "\n",
    "        out = self.minibatch_std(out)\n",
    "        return self.final_block(out).view(out.shape[0],-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\2a\\projet_pfa\\adngan\\AdnGAN\\ProGAN.py:122: UserWarning: std(): degrees of freedom is <= 0. Correction should be strictly less than the reduction factor (input numel divided by output numel). (Triggered internally at ..\\aten\\src\\ATen\\native\\ReduceOps.cpp:1760.)\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Succes at image size :4\n",
      "Succes at image size :8\n",
      "Succes at image size :16\n",
      "Succes at image size :32\n",
      "Succes at image size :64\n",
      "Succes at image size :128\n",
      "Succes at image size :256\n",
      "Succes at image size :512\n",
      "Succes at image size :1024\n"
     ]
    }
   ],
   "source": [
    "Z_DIM = 50\n",
    "IN_CHANNELS = 256\n",
    "gen = Generator_pro(Z_DIM,IN_CHANNELS)\n",
    "critic = Discriminator_pro(IN_CHANNELS)\n",
    "\n",
    "for img_size in [4,8,16,32,64,128,256,512,1024]:\n",
    "    num_steps = int(log2(img_size/4))\n",
    "    x = torch.randn(1,Z_DIM,1,1)\n",
    "    z = gen(x,0.5,steps=num_steps)\n",
    "    assert z.shape == (1,3,img_size,img_size)\n",
    "    out = critic(z,alpha=0.5,steps=num_steps)\n",
    "    assert out.shape == (1,1)\n",
    "    print(f\"Succes at image size :{img_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import cv2\n",
    "import torch\n",
    "from math import log2\n",
    "\n",
    "START_TRAIN_AT_IMG_SIZE = 128\n",
    "CHECKPOINT_GEN = \"gen_pro.pth\"\n",
    "CHECKPOINT_CRITIC = \"cri_pro.pth\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SAVE_MODEL = True\n",
    "LOAD_MODEL = False\n",
    "LEARNING_RATE = 1e-3\n",
    "BATCH_SIZES = [32, 32, 32, 16, 16, 16, 16, 8, 4]\n",
    "IMAGE_SIZE = 512\n",
    "CHANNELS_IMG = 1\n",
    "Z_DIM = 256  # should be 512 in original paper\n",
    "IN_CHANNELS = 256  # should be 512 in original paper\n",
    "CRITIC_ITERATIONS = 1\n",
    "LAMBDA_GP = 10\n",
    "NUM_STEPS = int(log2(IMAGE_SIZE/4))+1\n",
    "PROGRESSIVE_EPOCHS = [20] * len(BATCH_SIZES)\n",
    "FIXED_NOISE = torch.randn(8, Z_DIM, 1, 1).to(DEVICE)\n",
    "NUM_WORKERS = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loder(image_size):\n",
    "    return get_loader(channels_img=CHANNELS_IMG, image_size=IMAGE_SIZE, batch_size=BATCH_SIZES[int(log2(image_size / 4))]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(\n",
    "        critic,\n",
    "        gen,\n",
    "        loader,\n",
    "        dataset,\n",
    "        step,\n",
    "        alpha,\n",
    "        opt_critic,\n",
    "        opt_gen,\n",
    "        tensorboard_step,\n",
    "        writer,\n",
    "        scaler_gen,\n",
    "        scaler_critic,\n",
    "    ):\n",
    "    loop = tqdm(loader, leave=True)\n",
    "    for batch_idx,(real,_) in enumerate(loop):\n",
    "        real = real.to(DEVICE)\n",
    "        cur_batch_size = real.shape[0]\n",
    "\n",
    "        #Train Critic : max E[critic(real)] - E[critic(fake)] <-> min -E[critic(real)] + E[critic(fake)]\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    gen = Generator_pro(Z_DIM, IN_CHANNELS,CHANNELS_IMG).to(DEVICE)\n",
    "    critic = Discriminator_pro(IN_CHANNELS,CHANNELS_IMG).to(DEVICE)\n",
    "\n",
    "    #initialise optimizers and scaler for FP16 training\n",
    "    opt_gen = torch.optim.Adam(gen.parameters(),lr=LEARNING_RATE,betas=(0.0,0.99))\n",
    "    opt_critic = torch.optim.Adam(critic.parameters(),lr=LEARNING_RATE,betas=(0.0,0.99))\n",
    "    scaler_gen = torch.cuda.amp.GradScaler()\n",
    "    scaler_critic = torch.cuda.amp.GradScaler()\n",
    "\n",
    "    #for tensorboard plotting\n",
    "    writer = SummaryWriter(f\"logs/ProGAN\")\n",
    "\n",
    "    if LOAD_MODEL:\n",
    "        tools.load_checkpoint(CHECKPOINT_GEN,gen,opt_gen,scaler_gen,LEARNING_RATE)\n",
    "        tools.load_checkpoint(CHECKPOINT_CRITIC,critic,opt_critic,scaler_critic,LEARNING_RATE)\n",
    "\n",
    "    gen.train()\n",
    "    critic.train()\n",
    "    tensorboard_step = 0\n",
    "    step =int(log2(START_TRAIN_AT_IMG_SIZE/4)) \n",
    "\n",
    "    for num_epochs in PROGRESSIVE_EPOCHS[step:]:\n",
    "        alpha = 1e-5\n",
    "        loader,dataset = get_loder(4*(2**step))\n",
    "        print(f\"Working on img size : {4*(2**step)}\")\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}]\")\n",
    "            tensorboard_step,alpha = train_fn(\n",
    "                critic,\n",
    "                gen,\n",
    "                loader,\n",
    "                dataset,\n",
    "                step,\n",
    "                alpha,\n",
    "                opt_critic,\n",
    "                opt_gen,\n",
    "                tensorboard_step,\n",
    "                writer,\n",
    "                scaler_gen,\n",
    "                scaler_critic,\n",
    "            )\n",
    "\n",
    "            if SAVE_MODEL:\n",
    "                tools.save_checkpoint(gen,opt_gen,scaler_gen,filename=CHECKPOINT_GEN)\n",
    "                tools.save_checkpoint(critic,opt_critic,scaler_critic,filename=CHECKPOINT_CRITIC)\n",
    "\n",
    "            step += 1\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
